{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaGcb3sP2sp7"
      },
      "source": [
        "---\n",
        "\n",
        " ðŸ“Œ Call Transcript Search Agent (RULER-Based Evaluation)\n",
        "\n",
        "In this notebook, you will use the **Agent Reinforcement Trainer) (ART)** framework to train an agent that answers questions about **banking call transcripts**.\n",
        "\n",
        "The agent learns by performing tool-using rollouts and is scored with **RULER**, a robust relative ranking evaluator that helps the model learn to answer more accurately over time.\n",
        "\n",
        " ðŸ”§ Key Tools Available to the Agent\n",
        "\n",
        "1. `search_inbox` â€” Search for calls using keywords.  \n",
        "2. `read_call` â€” Read the full transcript of a specific call by ID.  \n",
        "3. `return_final_answer` â€” Return the final answer and the IDs of calls used.\n",
        "\n",
        " ðŸ“† Training Overview\n",
        "\n",
        "You will train the model in small batches (groups) of scenarios. Each scenario has several stochastic rollouts, which RULER scores, and those scores inform the gradient updates.\n",
        "\n",
        "ART will also periodically run **validation** rollouts and log the trajectories.\n",
        "\n",
        "---\n",
        "\n",
        " ðŸš€ Ready to Train?\n",
        "\n",
        "> To train this agent, click **Runtime > Run all**.  \n",
        "> Donâ€™t forget to set your environment variables:  \n",
        "> - `OPENAI_API_KEY`  \n",
        "> - `HF_TOKEN` (for dataset access)  \n",
        "> - `WANDB_API_KEY` (for logging, optional)\n",
        "\n",
        "This notebook uses:\n",
        "- **call transcripts** with multi-turn formats like `\"Customer: ... | Agent: ...\"`\n",
        "- **RULER** for scoring training rollouts\n",
        "- **ARTâ€™s training loop** with GRPO reinforcement learning\n",
        "\n",
        "Letâ€™s get started!\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W5KgcGs12sp9"
      },
      "source": [
        "### Installation\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "!pip install pyarrow datasets\n",
        "#uv would fail to install these in macOS due to arrow-cpp missing components"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3QGntHJm2sp9"
      },
      "outputs": [],
      "source": [
        "!uv pip install openpipe-art==0.5.2 langchain-core tenacity transformers peft accelerate safetensors huggingface_hub wandb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import json\n",
        "from typing import Dict, Any, List, Literal, Optional\n",
        "\n",
        "from datasets import DatasetDict, Dataset, Features, Sequence, Value, load_dataset\n",
        "from huggingface_hub import HfApi\n",
        "from openai import OpenAI\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "import random\n",
        "from dataclasses import dataclass, asdict\n",
        "from textwrap import dedent\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "import sqlite3\n",
        "from datetime import datetime\n",
        "\n",
        "import logging\n",
        "import json\n",
        "from textwrap import dedent\n",
        "\n",
        "from openai import AsyncOpenAI\n",
        "from litellm import acompletion\n",
        "import weave\n",
        "from langchain_core.utils.function_calling import convert_to_openai_tool\n",
        "from tenacity import retry, stop_after_attempt\n",
        "\n",
        "from pydantic import BaseModel, Field\n",
        "\n",
        "import art\n",
        "from art.utils.strip_logprobs import strip_logprobs\n",
        "\n",
        "from dotenv import load_dotenv\n",
        "\n",
        "import art\n",
        "from art.serverless.backend import ServerlessBackend\n",
        "\n",
        "import os, re, shutil, tempfile\n",
        "import wandb\n",
        "from huggingface_hub import login\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel\n",
        "\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "random.seed(117)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSNM_C5S2sp9"
      },
      "source": [
        "<a name=\"Environment-Variables\"></a>\n",
        "\n",
        "### Environment Variables\n",
        "\n",
        "Later on in the notebook, we'll be creating a model that can automatically logs metrics to Weights & Biases and chat completions to Weave. We'll also be using W&B Training for training and inference. In order to do so, you'll need to provide your Weights & Biases API key as an environment variable.\n",
        "\n",
        "**OpenAI (used for RULER judge model)**\n",
        "\n",
        "Our RULER reward function queries third-party models to judge the quality of the agent's performance. Any model supported by LiteLLM works. For this example we'll use OpenAI's o4-mini model, so we'll need to set the `OPENAI_API_KEY` environment variable."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CVxXeQXp2sp9"
      },
      "outputs": [],
      "source": [
        "# Required for RULER judge model\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"\"\n",
        "\n",
        "# Required for Weights & Biases\n",
        "os.environ[\"WANDB_API_KEY\"] = \"\"\n",
        "\n",
        "# Required for Hugging Face Hub\n",
        "os.environ[\"HF_TOKEN\"] = \"\"\n",
        "\n",
        "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
        "    raise ValueError(\n",
        "        \"OPENAI_API_KEY is required for RULER functionality when using openai/o4-mini.\"\n",
        "    )\n",
        "\n",
        "if not os.environ.get(\"WANDB_API_KEY\"):\n",
        "    raise ValueError(\"WANDB_API_KEY is required for inference, training, and logging to Weights & Biases.\")\n",
        "\n",
        "if not os.environ.get(\"HF_TOKEN\"):\n",
        "    raise ValueError(\"HF_TOKEN is required for uploading datasets to the Hugging Face Hub.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Dataset Generation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We utilize OpenAI models in order to create a dataset for our task. The dataset consists of transcripts as well as questions and answer pairs. The question is what the user asks(ISSUES AND COMPLAINTS WITH THEIR BANK PRODUCTS), e.g. what are the credit issues the customer faced. The answer is essentially the LLM reply, where it identifies the issues and complaints and provides context."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# ##  Synthetic Dataset Generation using OpenAI LLM and Upload to Hugging Face with QNA pairs\n",
        "\n",
        "# # ==== CONFIG ====\n",
        "# TARGET_REPO_ID = \"venetis/banking_calls_qna\"  # change if you like\n",
        "# NUM_EXAMPLES = 1000                           # how many synthetic calls\n",
        "# MODEL_NAME = \"gpt-4.1-mini\"                   # cost-effective, adjust if needed\n",
        "\n",
        "\n",
        "# client = OpenAI()\n",
        "# api = HfApi()\n",
        "\n",
        "\n",
        "# def build_generation_prompt() -> str:\n",
        "#     return \"\"\"\n",
        "# You are generating synthetic banking call-center data.\n",
        "\n",
        "# For ONE example, do the following:\n",
        "\n",
        "# 1. Invent a realistic phone call between:\n",
        "#    - a CUSTOMER (member of a bank or credit union)\n",
        "#    - a SUPPORT AGENT\n",
        "\n",
        "# 2. The main topic of the call must be a banking PRODUCT ISSUE, such as:\n",
        "#    - CREDIT CARD (declined transactions, fraud alerts, wrong fees, interest rate, limit issues, replacement card, etc.)\n",
        "#    - DEBIT CARD (declines, ATM issues, lost card, etc.)\n",
        "#    - CHECKING or SAVINGS ACCOUNT (fees, overdraft, holds, missing deposits, etc.)\n",
        "#    - LOAN or MORTGAGE (payment issues, wrong amount, interest confusion, payoff questions, etc.)\n",
        "\n",
        "# 3. Structure of the call:\n",
        "#    - At least 8 turns, at most 20 turns total.\n",
        "#    - Alternate between CUSTOMER and AGENT.\n",
        "#    - Around 10%â€“30% of the call should be small talk or off-topic (e.g., weather, \"how are you\", minor chit-chat).\n",
        "#    - The rest should be focused on the main PRODUCT ISSUE and how it is investigated or resolved.\n",
        "#    - The issue should be clearly understandable from the call.\n",
        "\n",
        "# 4. Formatting of the transcript:\n",
        "#    - Each turn must be formatted as: \"Customer: ...\"\n",
        "#      or \"Agent: ...\"\n",
        "#    - TURNS MUST BE SEPARATED BY THE PIPE CHARACTER \" | \".\n",
        "#      Example: \"Customer: ... | Agent: ... | Customer: ...\"\n",
        "#    - Do NOT include any newline characters in the transcript.\n",
        "#    - Do NOT include the \"|\" character inside the spoken text itself,\n",
        "#      only as the separator between turns.\n",
        "\n",
        "# 5. Question and answer:\n",
        "#    - Based on the transcript, write ONE natural-language QUESTION someone might ask\n",
        "#      about the PRODUCT ISSUE in the call. Examples:\n",
        "#        - \"What were the customer's credit card complaints?\"\n",
        "#        - \"Why was the member's debit card repeatedly declined?\"\n",
        "#        - \"What problem did the member report with their mortgage payment and how was it resolved?\"\n",
        "#    - The QUESTION must be answerable solely from the transcript.\n",
        "#    - Write a short, factual ANSWER that directly answers the question,\n",
        "#      summarizing the relevant parts of the call.\n",
        "\n",
        "# 6. Output format:\n",
        "#    Return ONLY a single JSON object with EXACTLY these keys:\n",
        "#    - \"question\": string\n",
        "#    - \"answer\"  : string\n",
        "#    - \"transcript\": string\n",
        "\n",
        "# Example of the REQUIRED JSON SHAPE (NOTE: This is just a SHAPE example, not content):\n",
        "\n",
        "# {\n",
        "#   \"question\": \"What issue did the customer have with their credit card?\",\n",
        "#   \"answer\": \"Their credit card was repeatedly declined due to a fraud alert.\",\n",
        "#   \"transcript\": \"Customer: ... | Agent: ... | Customer: ...\"\n",
        "# }\n",
        "\n",
        "# Now generate ONE such JSON object.\n",
        "# \"\"\".strip()\n",
        "\n",
        "# def generate_one_example() -> Dict[str, Any]:\n",
        "#     \"\"\"Generate one (question, answer, transcript) triple via the LLM.\"\"\"\n",
        "#     prompt = build_generation_prompt()\n",
        "\n",
        "#     resp = client.chat.completions.create(\n",
        "#         model=MODEL_NAME,\n",
        "#         temperature=0.5,\n",
        "#         messages=[\n",
        "#             {\n",
        "#                 \"role\": \"system\",\n",
        "#                 \"content\": \"You are a careful data generator. You ALWAYS return valid JSON with the exact fields requested.\",\n",
        "#             },\n",
        "#             {\"role\": \"user\", \"content\": prompt},\n",
        "#         ],\n",
        "#     )\n",
        "\n",
        "#     content = resp.choices[0].message.content\n",
        "\n",
        "#     # Try to parse as JSON; if it fails, we skip this example.\n",
        "#     try:\n",
        "#         data = json.loads(content)\n",
        "#     except json.JSONDecodeError:\n",
        "#         print(\"âš ï¸ JSON parse error, skipping one example.\")\n",
        "#         return {}\n",
        "\n",
        "#     # Basic validation\n",
        "#     q = data.get(\"question\", \"\").strip()\n",
        "#     a = data.get(\"answer\", \"\").strip()\n",
        "#     t = data.get(\"transcript\", \"\").strip()\n",
        "\n",
        "#     if not q or not a or not t:\n",
        "#         return {}\n",
        "\n",
        "#     # Sanity check: ensure pipe-separated turns\n",
        "#     if \"|\" not in t:\n",
        "#         print(\"âš ï¸ No '|' in transcript, skipping.\")\n",
        "#         return {}\n",
        "\n",
        "#     return {\n",
        "#         \"question\": q,\n",
        "#         \"answer\": a,\n",
        "#         \"transcript\": t,\n",
        "#     }\n",
        "\n",
        "# def generate_dataset(num_examples: int) -> Dataset:\n",
        "#     rows: List[Dict[str, Any]] = []\n",
        "\n",
        "#     for _ in tqdm(range(num_examples), desc=\"Generating synthetic calls\"):\n",
        "#         ex = generate_one_example()\n",
        "#         if ex:\n",
        "#             rows.append(ex)\n",
        "\n",
        "#     print(f\"Generated {len(rows)} valid examples out of {num_examples} attempts.\")\n",
        "\n",
        "#     if not rows:\n",
        "#         raise RuntimeError(\"No valid examples generated. Check prompts / API keys.\")\n",
        "\n",
        "#     ds = Dataset.from_list(rows)\n",
        "\n",
        "#     # Optional: add an integer ID\n",
        "#     ds = ds.map(lambda x, idx: {\"id\": idx}, with_indices=True)\n",
        "\n",
        "#     return ds\n",
        "\n",
        "\n",
        "# ds = generate_dataset(NUM_EXAMPLES)\n",
        "\n",
        "# def push_to_hf(ds: Dataset, repo_id: str):\n",
        "#     # Split into train/test if you like\n",
        "#     ds_train = ds.select(range(int(0.9 * len(ds))))\n",
        "#     ds_test  = ds.select(range(int(0.9 * len(ds)), len(ds)))\n",
        "\n",
        "#     dsd = DatasetDict({\"train\": ds_train, \"test\": ds_test})\n",
        "\n",
        "#     # You must be logged in or have HF_TOKEN set\n",
        "#     dsd.push_to_hub(repo_id)\n",
        "#     print(f\"Pushed dataset to: {repo_id}\")\n",
        "\n",
        "\n",
        "# push_to_hf(ds, TARGET_REPO_ID)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from datasets import load_dataset, DatasetDict\n",
        "\n",
        "# # Your current dataset with only a \"train\" split\n",
        "# REPO_ID = \"venetis/banking_calls_qna\"   \n",
        "# # Load the single split\n",
        "# ds_full = load_dataset(REPO_ID, split=\"train\")\n",
        "\n",
        "# # Shuffle for randomness\n",
        "# ds_full = ds_full.shuffle(seed=42)\n",
        "\n",
        "# # 90% train, 10% test\n",
        "# n = len(ds_full)\n",
        "# train_size = int(0.9 * n)\n",
        "\n",
        "# ds_train = ds_full.select(range(train_size))\n",
        "# ds_test  = ds_full.select(range(train_size, n))\n",
        "\n",
        "# print(\"Train size:\", len(ds_train))\n",
        "# print(\"Test size:\", len(ds_test))\n",
        "\n",
        "# # Create split dict and push back to HF\n",
        "# ds_dict = DatasetDict({\"train\": ds_train, \"test\": ds_test})\n",
        "# ds_dict.push_to_hub(REPO_ID)\n",
        "\n",
        "# print(f\"âœ… Updated {REPO_ID} with train + test splits.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "{\"id\": ..., \n",
        "\n",
        "\"question\": ..., \n",
        "\n",
        "\"answer\": ..., \n",
        "\n",
        "\"transcript\": ...}\n",
        "\n",
        "where transcript is a multi-turn call like\n",
        "Customer: ... | Agent: ... | Customer: ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_b2K9qBi2sp9"
      },
      "source": [
        "<a name=\"Environment\"></a>\n",
        "\n",
        "### Call Search Environment\n",
        "\n",
        "ART allows your agent to learn by interacting with its environment.  \n",
        "In this example, we create an environment where the agent can **search through call transcripts and answer questions about them**.\n",
        "\n",
        "The agent will have access to three tools:\n",
        "\n",
        "1. `search_calls` â€“ Search for calls by keywords in their transcript using a full-text index.\n",
        "2. `read_call` â€“ Read a specific call by its call ID (returns the full multi-turn transcript).\n",
        "3. `return_final_answer` â€“ Return the final answer along with the source call IDs that were used.\n",
        "\n",
        "The questions will typically be about **banking products and issues**, e.g.:\n",
        "\n",
        "> *â€œWhat were the credit card complaints?â€*  \n",
        "> *â€œWhy was the memberâ€™s debit card repeatedly declined?â€*\n",
        "\n",
        "The agent must learn to:\n",
        "\n",
        "- translate such questions into the right **search queries** over the call transcripts,\n",
        "- read the retrieved calls via `read_call`,\n",
        "- and produce a concise, grounded answer via `return_final_answer`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# Call and Scenario data models\n",
        "# ============================================================\n",
        "max_results = 20 # max search results to return #TODO\n",
        "\n",
        "\n",
        "class Call(BaseModel):\n",
        "    \"\"\"\n",
        "    Representation of a call in our local SQLite database.\n",
        "\n",
        "    NOTE:\n",
        "    - Our HF dataset only has an ID and a transcript (multi-turn dialog).\n",
        "    \"\"\"\n",
        "    call_id: str                 # string ID for the call (derived from `id` in HF)\n",
        "    transcript: str              # full multi-turn text: \"Customer: ... | Agent: ... | ...\"\n",
        "\n",
        "\n",
        "class Scenario(BaseModel):\n",
        "    \"\"\"\n",
        "    Scenario used for training / evaluation, one row per HF example.\n",
        "\n",
        "    Our HF dataset schema is:\n",
        "\n",
        "        {\n",
        "          \"id\": int,\n",
        "          \"question\": str,\n",
        "          \"answer\": str,\n",
        "          \"transcript\": str\n",
        "        }\n",
        "\n",
        "    We add a `split` field when loading (train / test)\n",
        "    \"\"\"\n",
        "    id: int\n",
        "    question: str\n",
        "    answer: str\n",
        "    transcript: str\n",
        "    split: Literal[\"train\", \"test\"]  # added when we call load_scenarios\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class SearchResult:\n",
        "    \"\"\"\n",
        "    Search result for DB lookups.\n",
        "    \"\"\"\n",
        "    message_id: str   # = call_id in the calls table\n",
        "    snippet: str      # short excerpt from transcript\n",
        "\n",
        "\n",
        "class FinalAnswer(BaseModel):\n",
        "    \"\"\"\n",
        "    Final answer object used later by the agent (same shape as original).\n",
        "    \"\"\"\n",
        "    answer: str\n",
        "    source_ids: list[str]\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Database configuration\n",
        "# ============================================================\n",
        "\n",
        "# Local SQLite DB with all call transcripts (for FTS search)\n",
        "DB_PATH = \"./calls.db\"\n",
        "CALL_DATASET_REPO_ID = \"venetis/banking_calls_qna\"\n",
        "\n",
        "# Global database connection, reused across calls\n",
        "db_conn = None\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Database creation: from HF calls â†’ SQLite + FTS\n",
        "# ============================================================\n",
        "\n",
        "def create_calls_database():\n",
        "    \"\"\"\n",
        "    Create the call database from the Hugging Face dataset.\n",
        "\n",
        "    - One table: calls(id, call_id, transcript)\n",
        "    - One FTS5 virtual table: calls_fts(transcript) with triggers\n",
        "      to keep it in sync with calls.\n",
        "\n",
        "    We index ALL calls (no limit), deduplicating on (call_id, transcript).\n",
        "    \"\"\"\n",
        "    print(\"Creating calls database from Hugging Face dataset...\")\n",
        "    print(\n",
        "        \"This will download and process the full call dataset. \"\n",
        "        \"It may take a few minutes the first time...\"\n",
        "    )\n",
        "\n",
        "    # -----------------------------\n",
        "    # Database schema\n",
        "    # -----------------------------\n",
        "    SQL_CREATE_TABLES = \"\"\"\n",
        "    DROP TABLE IF EXISTS calls;\n",
        "    DROP TABLE IF EXISTS calls_fts;\n",
        "\n",
        "    CREATE TABLE calls (\n",
        "        id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
        "        call_id TEXT UNIQUE,\n",
        "        transcript TEXT\n",
        "    );\n",
        "    \"\"\"\n",
        "\n",
        "    SQL_CREATE_INDEXES_TRIGGERS = \"\"\"\n",
        "    CREATE INDEX idx_calls_call_id ON calls(call_id);\n",
        "\n",
        "    CREATE VIRTUAL TABLE calls_fts USING fts5(\n",
        "        transcript,\n",
        "        content='calls',\n",
        "        content_rowid='id'\n",
        "    );\n",
        "\n",
        "    CREATE TRIGGER calls_ai AFTER INSERT ON calls BEGIN\n",
        "        INSERT INTO calls_fts (rowid, transcript)\n",
        "        VALUES (new.id, new.transcript);\n",
        "    END;\n",
        "\n",
        "    CREATE TRIGGER calls_ad AFTER DELETE ON calls BEGIN\n",
        "        DELETE FROM calls_fts WHERE rowid=old.id;\n",
        "    END;\n",
        "\n",
        "    CREATE TRIGGER calls_au AFTER UPDATE ON calls BEGIN\n",
        "        UPDATE calls_fts SET transcript=new.transcript WHERE rowid=old.id;\n",
        "    END;\n",
        "    \"\"\"\n",
        "\n",
        "    # -----------------------------\n",
        "    # Create empty DB\n",
        "    # -----------------------------\n",
        "    conn = sqlite3.connect(DB_PATH)\n",
        "    cursor = conn.cursor()\n",
        "    cursor.executescript(SQL_CREATE_TABLES)\n",
        "    conn.commit()\n",
        "\n",
        "    # -----------------------------\n",
        "    # Load HF dataset\n",
        "    # -----------------------------\n",
        "    print(\"Loading call dataset from Hugging Face...\")\n",
        "\n",
        "    # We only really need 'id' and 'transcript', but we define expected features\n",
        "    # to make things explicit and robust to types.\n",
        "    expected_features = Features(\n",
        "        {\n",
        "            \"id\": Value(\"int64\"),\n",
        "            \"question\": Value(\"string\"),\n",
        "            \"answer\": Value(\"string\"),\n",
        "            \"transcript\": Value(\"string\"),\n",
        "        }\n",
        "    )\n",
        "\n",
        "    dataset = load_dataset(\n",
        "        CALL_DATASET_REPO_ID,\n",
        "        features=expected_features,\n",
        "        split=\"train\",\n",
        "    )\n",
        "\n",
        "    print(f\"Dataset contains {len(dataset)} total call examples\")\n",
        "\n",
        "    # -----------------------------\n",
        "    # Populate DB with all calls\n",
        "    # -----------------------------\n",
        "    print(\"Populating database with call transcripts...\")\n",
        "    conn.execute(\"PRAGMA synchronous = OFF;\")\n",
        "    conn.execute(\"PRAGMA journal_mode = MEMORY;\")\n",
        "    conn.execute(\"BEGIN TRANSACTION;\")\n",
        "\n",
        "    record_count = 0\n",
        "    skipped_count = 0\n",
        "    duplicate_count = 0\n",
        "\n",
        "    # Track (call_id, transcript) to avoid duplicates\n",
        "    processed_calls = set()\n",
        "\n",
        "    for call_data in tqdm(dataset, desc=\"Inserting calls\"):\n",
        "        # HF \"id\" â†’ string call_id; transcript is required\n",
        "        raw_id = call_data[\"id\"]\n",
        "        call_id = str(raw_id)\n",
        "        transcript = call_data[\"transcript\"]\n",
        "\n",
        "        if not transcript or not transcript.strip():\n",
        "            skipped_count += 1\n",
        "            continue\n",
        "\n",
        "        # Optional: filter out absurdly long transcripts \n",
        "        if len(transcript) > 20000:\n",
        "            skipped_count += 1\n",
        "            continue\n",
        "\n",
        "        call_key = (call_id, transcript)\n",
        "        if call_key in processed_calls:\n",
        "            duplicate_count += 1\n",
        "            continue\n",
        "        processed_calls.add(call_key)\n",
        "\n",
        "        cursor.execute(\n",
        "            \"\"\"\n",
        "            INSERT INTO calls (call_id, transcript)\n",
        "            VALUES (?, ?)\n",
        "        \"\"\",\n",
        "            (call_id, transcript),\n",
        "        )\n",
        "\n",
        "        record_count += 1\n",
        "\n",
        "    conn.commit()\n",
        "\n",
        "    # -----------------------------\n",
        "    # Create FTS index and rebuild\n",
        "    # -----------------------------\n",
        "    print(\"Creating FTS index and triggers...\")\n",
        "    cursor.executescript(SQL_CREATE_INDEXES_TRIGGERS)\n",
        "    cursor.execute('INSERT INTO calls_fts(calls_fts) VALUES(\"rebuild\")')\n",
        "    conn.commit()\n",
        "\n",
        "    print(f\"Successfully created calls database with {record_count} rows.\")\n",
        "    print(f\"Skipped {skipped_count} rows (empty/too long).\")\n",
        "    print(f\"Skipped {duplicate_count} duplicate calls.\")\n",
        "    return conn\n",
        "\n",
        "\n",
        "def get_db_connection():\n",
        "    \"\"\"\n",
        "    Get a (global) database connection.\n",
        "\n",
        "    - If DB already exists, open it.\n",
        "    - If not, build it from the HF dataset.\n",
        "    \"\"\"\n",
        "    global db_conn\n",
        "    if db_conn is None:\n",
        "        if os.path.exists(DB_PATH):\n",
        "            print(f\"Loading existing call database from {DB_PATH}\")\n",
        "            db_conn = sqlite3.connect(DB_PATH, check_same_thread=False)\n",
        "        else:\n",
        "            db_conn = create_calls_database()\n",
        "    return db_conn\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Search & read helpers (used later by the agent tools)\n",
        "# ============================================================\n",
        "\n",
        "def search_calls(\n",
        "    keywords: List[str],\n",
        "    max_results: int = 10,\n",
        ") -> List[SearchResult]:\n",
        "    \"\"\"\n",
        "    Search the call database based on keywords in the transcript.\n",
        "\n",
        "    \"\"\"\n",
        "    conn = get_db_connection()\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    where_clauses: List[str] = []\n",
        "    params: List[str | int] = []\n",
        "\n",
        "    if not keywords:\n",
        "        raise ValueError(\"No keywords provided for search.\")\n",
        "\n",
        "    if max_results > max_results:\n",
        "        raise ValueError(f\"max_results must be less than or equal to {max_results}.\")\n",
        "\n",
        "    # FTS5 default is AND between terms.\n",
        "    # We escape quotes and wrap each keyword in quotes for safety.\n",
        "    fts_query = \" \".join(f\"\"\" \"{k.replace('\"', '\"\"')}\" \"\"\" for k in keywords)\n",
        "    where_clauses.append(\"fts.calls_fts MATCH ?\")\n",
        "    params.append(fts_query)\n",
        "\n",
        "    sql = f\"\"\"\n",
        "        SELECT\n",
        "            c.call_id,\n",
        "            snippet(calls_fts, 0, '<b>', '</b>', ' ... ', 15) as snippet\n",
        "        FROM\n",
        "            calls c JOIN calls_fts fts ON c.id = fts.rowid\n",
        "        WHERE\n",
        "            {\" AND \".join(where_clauses)}\n",
        "        LIMIT ?;\n",
        "    \"\"\"\n",
        "    params.append(max_results)\n",
        "\n",
        "    cursor.execute(sql, params)\n",
        "    results = cursor.fetchall()\n",
        "\n",
        "    # We reuse the field name `message_id` in SearchResult for compatibility\n",
        "    return [SearchResult(message_id=row[0], snippet=row[1]) for row in results]\n",
        "\n",
        "\n",
        "def read_call(call_id: str) -> Optional[Call]:\n",
        "    \"\"\"\n",
        "    Retrieve a single call by its call_id from the DB.\n",
        "    This is the analog of read_email.\n",
        "    \"\"\"\n",
        "    conn = get_db_connection()\n",
        "    cursor = conn.cursor()\n",
        "\n",
        "    cursor.execute(\n",
        "        \"SELECT call_id, transcript FROM calls WHERE call_id = ?\",\n",
        "        (call_id,),\n",
        "    )\n",
        "    row = cursor.fetchone()\n",
        "\n",
        "    if not row:\n",
        "        return None\n",
        "\n",
        "    call_id, transcript = row\n",
        "    return Call(call_id=call_id, transcript=transcript)\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Scenario loader (from HF Q&A dataset)\n",
        "# ============================================================\n",
        "\n",
        "def load_scenarios(\n",
        "    split: Literal[\"train\", \"test\"] = \"train\",\n",
        "    limit: Optional[int] = None,\n",
        "    shuffle: bool = True,\n",
        "    seed: Optional[int] = None,\n",
        ") -> List[Scenario]:\n",
        "    \"\"\"\n",
        "    Load scenarios from the Hugging Face Q&A dataset.\n",
        "\n",
        "    HF SCHEMA:\n",
        "        {\n",
        "          \"id\": int,\n",
        "          \"question\": str,\n",
        "          \"answer\": str,\n",
        "          \"transcript\": str\n",
        "        }\n",
        "\n",
        "    \"\"\"\n",
        "    print(f\"Loading {split} scenarios from Hugging Face...\")\n",
        "    dataset: Dataset = load_dataset(CALL_DATASET_REPO_ID, split=split)\n",
        "\n",
        "    if shuffle or (seed is not None):\n",
        "        if seed is not None:\n",
        "            dataset = dataset.shuffle(seed=seed)\n",
        "        else:\n",
        "            dataset = dataset.shuffle()\n",
        "\n",
        "    # Convert each row to a Scenario object\n",
        "    scenarios = [\n",
        "        Scenario(\n",
        "            id=int(row[\"id\"]),\n",
        "            question=str(row[\"question\"]),\n",
        "            answer=str(row[\"answer\"]),\n",
        "            transcript=str(row[\"transcript\"]),\n",
        "            split=split,\n",
        "        )\n",
        "        for row in dataset\n",
        "    ]\n",
        "\n",
        "    if shuffle:\n",
        "        if seed is not None:\n",
        "            rng = random.Random(seed)\n",
        "            rng.shuffle(scenarios)\n",
        "        else:\n",
        "            random.shuffle(scenarios)\n",
        "\n",
        "    if limit is not None:\n",
        "        scenarios = scenarios[:limit]\n",
        "\n",
        "    print(f\"Loaded {len(scenarios)} {split} scenarios.\")\n",
        "    return scenarios\n",
        "\n",
        "\n",
        "# ============================================================\n",
        "# Load training & validation scenarios and print a sample\n",
        "# ============================================================\n",
        "\n",
        "training_scenarios = load_scenarios(\n",
        "    split=\"train\",\n",
        "    # limit=50,           \n",
        "    shuffle=True,\n",
        "    seed=117,\n",
        ")\n",
        "\n",
        "validation_scenarios = load_scenarios(\n",
        "    split=\"test\",\n",
        "    # limit=20,\n",
        "    shuffle=True,\n",
        "    seed=117,\n",
        ")\n",
        "\n",
        "print(\"Call search environment created with full call dataset!\")\n",
        "print(\n",
        "    f\"Database contains the complete call dataset, \"\n",
        "    f\"loaded {len(training_scenarios)} training scenarios and \"\n",
        "    f\"{len(validation_scenarios)} validation scenarios.\"\n",
        ")\n",
        "\n",
        "# Print first scenario to sanity-check everything\n",
        "print(\"\\nSample scenario\")\n",
        "print(\"id:\", training_scenarios[0].id)\n",
        "print(\"question:\", training_scenarios[0].question)\n",
        "print(\"answer:\", training_scenarios[0].answer)\n",
        "print(\"transcript (first 300 chars):\", training_scenarios[0].transcript[:300] + \"...\")\n",
        "print(\"split:\", training_scenarios[0].split)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "722Z1vJ32sp-"
      },
      "source": [
        "### Creating a Model\n",
        "\n",
        "Now that we've defined the rules of our environment, we can create a model that will learn to search call transcripts effectively. We'll use a Qwen 3 14B model for this example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DDPil3sr2sp-"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Declare the model\n",
        "model = art.TrainableModel(\n",
        "    project=\"call-transcript-search-agent\",\n",
        "    name=\"call-transcript-agent-whole-dataset-2\",\n",
        "    base_model=\"OpenPipe/Qwen3-14B-Instruct\",\n",
        ")\n",
        "\n",
        "# ============================================================\n",
        "# Initialize the Serverless backend\n",
        "# ------------------------------------------------------------\n",
        "# This tells ART to run both:\n",
        "#   - inference (rollouts) and\n",
        "#   - training (GRPO updates)\n",
        "# on Weights & Biases' serverless infrastructure.\n",
        "# ============================================================\n",
        "backend = ServerlessBackend()\n",
        "\n",
        "# ============================================================\n",
        "# Register the model with the Serverless backend\n",
        "# ------------------------------------------------------------\n",
        "# This:\n",
        "#   - creates an inference endpoint for the model\n",
        "#   - sets up logging & telemetry\n",
        "#   - prepares training resources for GRPO\n",
        "#\n",
        "# After this `await`, `model.inference_base_url` and\n",
        "# `model.inference_api_key` are ready and used inside rollout().\n",
        "# ============================================================\n",
        "await model.register(backend)\n",
        "\n",
        "print(\"âœ… Registered TrainableModel\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4xuLqXsy2sp-"
      },
      "source": [
        "### Defining a Rollout\n",
        "\n",
        "A rollout is a single episode of an agent performing its task. In this example,\n",
        "the rollout function presents the agent with a **call search scenario**, and the\n",
        "agent uses the available tools to:\n",
        "\n",
        "1. Search through call transcripts (`search_inbox` / `search_calls`)\n",
        "2. Read specific calls (`read_call_tool`)\n",
        "3. Return a final answer with the call IDs used as evidence (`return_final_answer`)\n",
        "\n",
        "When the agent provides a final answer, the `correct` metric is calculated based\n",
        "on whether the answer matches the reference answer for that scenario."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_JfMwQ0p2sp-",
        "outputId": "2617ed39-1c11-4004-817d-bd7f95745562"
      },
      "outputs": [],
      "source": [
        "\n",
        "# -------------------------------------------------------------------\n",
        "# Weave & logging setup \n",
        "# -------------------------------------------------------------------\n",
        "logging.getLogger(\"weave\").setLevel(logging.CRITICAL)\n",
        "\n",
        "weave.init(\n",
        "    model.project,\n",
        "    settings={\"print_call_link\": False},\n",
        "    # remove logprobs before recording in Weave\n",
        "    global_postprocess_output=strip_logprobs\n",
        ")\n",
        "\n",
        "# Decrease the number of turns to speed up training\n",
        "MAX_TURNS = 6\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 2. Trajectory + wrapper types for rollouts\n",
        "# -------------------------------------------------------------------\n",
        "class ProjectTrajectory(art.Trajectory):\n",
        "    final_answer: FinalAnswer | None = None\n",
        "\n",
        "\n",
        "class CallScenario(BaseModel):\n",
        "    \"\"\"\n",
        "    Wrapper passed into rollout by the training loop.\n",
        "    \"\"\"\n",
        "    step: int\n",
        "    scenario: Scenario\n",
        "\n",
        "\n",
        "# -------------------------------------------------------------------\n",
        "# 3. Rollout: agent interacts with call-search environment\n",
        "# -------------------------------------------------------------------\n",
        "@weave.op\n",
        "async def rollout(model: art.Model, call_scenario: CallScenario) -> ProjectTrajectory:\n",
        "    scenario = call_scenario.scenario\n",
        "\n",
        "    # Initialize an empty trajectory\n",
        "    traj = ProjectTrajectory(\n",
        "        reward=0.0,\n",
        "        messages_and_choices=[],\n",
        "        metadata={\n",
        "            \"scenario_id\": scenario.id,\n",
        "            \"step\": call_scenario.step,\n",
        "        },\n",
        "    )\n",
        "\n",
        "    # System prompt: call-search agent\n",
        "    system_prompt = dedent(\n",
        "        f\"\"\"\n",
        "        You are a call transcript search agent.\n",
        "\n",
        "        You are given:\n",
        "        - A user question about banking issues (credit cards, loans, accounts, etc.).\n",
        "        - Tools that let you:\n",
        "            1) search through a database of call transcripts by keywords, and\n",
        "            2) read the full text of a specific call by its ID,\n",
        "            3) return a final answer with the IDs of the calls you used.\n",
        "\n",
        "        Each call transcript is a multi-turn conversation with this format:\n",
        "\n",
        "            \"Customer: ... | Agent: ... | Customer: ... | ...\"\n",
        "\n",
        "        Your job:\n",
        "        1. Use the search tool to find calls relevant to the question.\n",
        "        2. Use the read tool to inspect promising calls in detail.\n",
        "        3. Identify which call(s) actually match the issue described in the question.\n",
        "        4. Provide a concise, factual answer grounded in the transcripts.\n",
        "        5. When you are done, call `return_final_answer` with:\n",
        "           - your final answer, and\n",
        "           - the list of call IDs you relied on.\n",
        "\n",
        "        You may take up to {MAX_TURNS} turns, so if your first search is not good\n",
        "        enough, refine your keywords and try again.\n",
        "        \"\"\"\n",
        "    )\n",
        "\n",
        "    # Seed the conversation with system + user question\n",
        "    traj.messages_and_choices = [\n",
        "        {\"role\": \"system\", \"content\": system_prompt},\n",
        "        {\"role\": \"user\", \"content\": scenario.question},\n",
        "    ]\n",
        "\n",
        "    # ----------------- Tool definitions -----------------\n",
        "\n",
        "    def search_inbox(keywords: List[str]) -> list[dict]:\n",
        "        \"\"\"\n",
        "        Search the calls database for transcripts matching the given keywords.\n",
        "\n",
        "        Returns a list of dictionaries so the LLM can reason over them.\n",
        "        Each dictionary will typically look like:\n",
        "            { \"message_id\": \"...\", \"snippet\": \"...\" }\n",
        "        \"\"\"\n",
        "        results = search_calls(\n",
        "            keywords=keywords,\n",
        "            max_results=max_results,\n",
        "        )\n",
        "        return [asdict(result) for result in results]\n",
        "\n",
        "    def return_final_answer(\n",
        "        answer: str, reference_message_ids: List[str]\n",
        "    ) -> \"FinalAnswer\":\n",
        "        \"\"\"\n",
        "        Return the final answer and the IDs of the calls that were used.\n",
        "\n",
        "        This is the termination signal for the rollout.\n",
        "        \"\"\"\n",
        "        return FinalAnswer(answer=answer, source_ids=reference_message_ids)\n",
        "\n",
        "    # Collect tools; read_call is assumed to be defined in the Environment cell\n",
        "    tools = [search_inbox, read_call, return_final_answer]\n",
        "    tools_by_name = {t.__name__: t for t in tools}\n",
        "    traj.tools = [convert_to_openai_tool(t) for t in tools]\n",
        "\n",
        "    # Async client to query the current policy via ART's inference endpoint\n",
        "    client = AsyncOpenAI(\n",
        "        base_url=model.inference_base_url,\n",
        "        api_key=model.inference_api_key,\n",
        "    )\n",
        "\n",
        "    # ----------------- Tool-calling loop -----------------\n",
        "    for _ in range(MAX_TURNS):\n",
        "        response = await client.chat.completions.create(\n",
        "            model=model.get_inference_name(),\n",
        "            temperature=0.7,\n",
        "            messages=traj.messages(),\n",
        "            tools=traj.tools,\n",
        "        )\n",
        "\n",
        "        response_message = response.choices[0].message\n",
        "        traj.messages_and_choices.append(response.choices[0])\n",
        "\n",
        "        # If the model didn't call any tools, trajectory ends here\n",
        "        if not response_message.tool_calls:\n",
        "            return traj\n",
        "\n",
        "        try:\n",
        "            for tool_call in response_message.tool_calls:\n",
        "                tool_name: str = tool_call.function.name\n",
        "                if tool_name in tools_by_name:\n",
        "                    tool_args = json.loads(tool_call.function.arguments or \"{}\")\n",
        "                    tool_to_call = tools_by_name[tool_name]\n",
        "                    result = tool_to_call(**tool_args)\n",
        "\n",
        "                    # Record tool output in the trajectory\n",
        "                    traj.messages_and_choices.append(\n",
        "                        {\n",
        "                            \"role\": \"tool\",\n",
        "                            \"tool_call_id\": tool_call.id,\n",
        "                            \"name\": tool_name,\n",
        "                            \"content\": str(result),\n",
        "                        }\n",
        "                    )\n",
        "\n",
        "                    # If the agent calls return_final_answer, we evaluate and stop\n",
        "                    if tool_name == \"return_final_answer\":\n",
        "                        traj.final_answer = result\n",
        "\n",
        "                        return traj\n",
        "        except Exception as e:\n",
        "            print(f\"Error executing tool call: {e}\")\n",
        "            return traj\n",
        "\n",
        "    # If we reach here, the agent used up all turns without a final answer\n",
        "    # Reward stays 0, but we still log the partial trajectory.\n",
        "    return traj\n",
        "\n",
        "\n",
        "print(\"âœ… Rollout + granular correctness grading defined for call transcripts.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "emEdV-0H2sp_"
      },
      "source": [
        "<a name=\"ruler\"></a>\n",
        "\n",
        "### How RULER works\n",
        "\n",
        "**RULER** leverages two key insights:\n",
        "\n",
        "1. Relative scoring is easier than absolute scoring: It's easier for an LLM to rank several solutions relative to each other than to score them in isolation\n",
        "2. GRPO only needs relative scores: Since GRPO normalizes scores within each group, only the relative rankings matter, not absolute values\n",
        "\n",
        "The process:\n",
        "\n",
        "1. Generate N trajectories for a given scenario\n",
        "2. Pass all N trajectories to **RULER**\n",
        "3. **RULER** deduplicates common prefixes (e.g., identical system messages)\n",
        "4. An LLM judge scores each trajectory from 0 to 1 based on goal achievement\n",
        "5. These scores are used directly as rewards in GRPO training\n",
        "\n",
        "To learn more about **RULER**, check out the [RULER docs](https://art.openpipe.ai/fundamentals/ruler).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LdxlofGp2sp_",
        "outputId": "7429e03e-8c86-48fd-8f95-aa770e38a764"
      },
      "outputs": [],
      "source": [
        "# import art\n",
        "# from art.rewards import ruler_score_group\n",
        "\n",
        "# # Test RULER with a simple example\n",
        "# base_messages = [\n",
        "#     {\"role\": \"system\", \"content\": \"You count numbers using numeric symbols.\"},\n",
        "#     {\"role\": \"user\", \"content\": \"Count to 10.\"},\n",
        "# ]\n",
        "\n",
        "# good_trajectory = art.Trajectory(\n",
        "#     messages_and_choices=[\n",
        "#         *base_messages,\n",
        "#         {\"role\": \"assistant\", \"content\": \"1, 2, 3, 4, 5, 6, 7, 8, 9, 10\"},\n",
        "#     ],\n",
        "#     reward=0,\n",
        "# )\n",
        "\n",
        "# mediocre_trajectory = art.Trajectory(\n",
        "#     messages_and_choices=[\n",
        "#         *base_messages,\n",
        "#         {\n",
        "#             \"role\": \"assistant\",\n",
        "#             \"content\": \"one, two, three, four, five, six, seven, eight, nine, ten\",\n",
        "#         },\n",
        "#     ],\n",
        "#     reward=0,\n",
        "# )\n",
        "\n",
        "# bad_trajectory = art.Trajectory(\n",
        "#     messages_and_choices=[\n",
        "#         *base_messages,\n",
        "#         {\"role\": \"assistant\", \"content\": \"a, b, c, d, e, f, g, h, i, j\"},\n",
        "#     ],\n",
        "#     reward=0,\n",
        "# )\n",
        "\n",
        "# sample_group = art.TrajectoryGroup(\n",
        "#     trajectories=[\n",
        "#         good_trajectory,\n",
        "#         mediocre_trajectory,\n",
        "#         bad_trajectory,\n",
        "#     ]\n",
        "# )\n",
        "\n",
        "# judged_group = await ruler_score_group(sample_group, \"openai/o4-mini\", debug=True)\n",
        "# assert judged_group is not None\n",
        "\n",
        "# # Display rankings\n",
        "# sorted_trajectories = sorted(\n",
        "#     judged_group.trajectories, key=lambda t: t.reward, reverse=True\n",
        "# )\n",
        "# for rank, traj in enumerate(sorted_trajectories, 1):\n",
        "#     messages = traj.messages()\n",
        "#     print(f\"\\nRank {rank}: Score {traj.reward:.3f}\")\n",
        "#     print(f\"  Response: {messages[-1]['content'][:50]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ujWX6KNk2sp_"
      },
      "source": [
        "<a name=\"Loop\"></a>\n",
        "\n",
        "### Training Loop\n",
        "\n",
        "The training loop is where the magic happens. For each of the 'max_steps' steps or 'epochs' defined below, the rollout function will be called multiple times in parallel. Each scenario will produce a trajectory, which will be used to update the model.\n",
        "\n",
        "The `gather` step will wait for all of the trajectories to be generated, then it will use RULER to assign relative scores to each trajectory.\n",
        "\n",
        "Our notebook will then delete all but the most recent checkpoint and train the model on the scored trajectories.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1BIiniUR2sp_"
      },
      "outputs": [],
      "source": [
        "# -------------------------------------------------------------------\n",
        "# Training loop for call-transcript search agent (RULER-only evaluation)\n",
        "# -------------------------------------------------------------------\n",
        "from art.utils import iterate_dataset\n",
        "from art.rewards import ruler_score_group\n",
        "import math\n",
        "\n",
        "\n",
        "training_config = {\n",
        "    # A \"group\" = one scenario with multiple stochastic rollouts\n",
        "    \"groups_per_step\": 3,\n",
        "    \"num_epochs\": 2,\n",
        "    \"rollouts_per_group\": 4,   # keep moderate to avoid backend overload\n",
        "    \"learning_rate\": 1e-5,\n",
        "    #\"max_steps\": 10,     #TODO      # keep small for demo; increase when stable\n",
        "    \"validation_step_interval\": 25,\n",
        "}\n",
        "\n",
        "# Iterator over training scenarios (HF dataset -> Scenario objects)\n",
        "training_iterator = iterate_dataset(\n",
        "    training_scenarios,\n",
        "    groups_per_step=training_config[\"groups_per_step\"],\n",
        "    num_epochs=training_config[\"num_epochs\"],\n",
        "    initial_step=await model.get_step(),\n",
        ")\n",
        "##########\n",
        "# Count total training scenarios you will iterate over:\n",
        "num_training = len(training_scenarios)\n",
        "\n",
        "# Grab your config values\n",
        "groups_per_step = training_config[\"groups_per_step\"]\n",
        "num_epochs = training_config[\"num_epochs\"]\n",
        "\n",
        "# Compute how many batches per epoch\n",
        "steps_per_epoch = math.ceil(num_training / groups_per_step)\n",
        "\n",
        "# Compute total planned training steps\n",
        "total_expected_steps = steps_per_epoch * num_epochs\n",
        "\n",
        "print(f\"Training scenarios: {num_training}\")\n",
        "print(f\"Groups per step:   {groups_per_step}\")\n",
        "print(f\"Epochs:            {num_epochs}\")\n",
        "print(f\"Steps per epoch:   {steps_per_epoch}\")\n",
        "print(f\"Total planned steps: {total_expected_steps}\")\n",
        "\n",
        "##########\n",
        "\n",
        "for batch in training_iterator:\n",
        "    if batch.step % training_config[\"validation_step_interval\"] == 0 and batch.step > 0:\n",
        "        print(\n",
        "            f\"Training step {batch.step}, epoch {batch.epoch}, epoch step {batch.epoch_step}\"\n",
        "        )\n",
        "        print(f\"Batch contains {len(batch.items)} scenarios\")\n",
        "    # ----------------- 1. Create trajectory groups -----------------\n",
        "    train_groups = []\n",
        "    for scenario in batch.items:\n",
        "        # IMPORTANT: pass a *dict* into CallScenario so Pydantic is happy\n",
        "        call_scenario = CallScenario(\n",
        "            step=batch.step,\n",
        "            scenario=scenario.model_dump(),  # <- key fix vs previous version\n",
        "        )\n",
        "\n",
        "        # For each scenario, create a TrajectoryGroup of multiple rollouts\n",
        "        # (same question, different stochastic generations).\n",
        "        train_groups.append(\n",
        "            art.TrajectoryGroup(\n",
        "                (\n",
        "                    rollout(model, call_scenario)\n",
        "                    for _ in range(training_config[\"rollouts_per_group\"])\n",
        "                )\n",
        "            )\n",
        "        )\n",
        "\n",
        "    # ----------------- 2. Gather trajectories from backend -----------------\n",
        "    finished_train_groups = await art.gather_trajectory_groups(\n",
        "        train_groups,\n",
        "        pbar_desc=\"gather-train\",\n",
        "        max_exceptions=training_config[\"rollouts_per_group\"] * len(batch.items),\n",
        "    )\n",
        "\n",
        "    # Filter out any empty groups (no successful trajectories)\n",
        "    finished_train_groups = [\n",
        "        g for g in finished_train_groups if len(g.trajectories) > 0\n",
        "    ]\n",
        "    if not finished_train_groups:\n",
        "        print(\"âš ï¸ No finished training groups for this batch, skipping step.\")\n",
        "        continue\n",
        "\n",
        "    # ----------------- 3. Apply RULER scoring within each group -----------------\n",
        "    judged_groups = []\n",
        "    for group in finished_train_groups:\n",
        "        # RULER re-scores trajectories in the group relatively.\n",
        "        # It uses the per-trajectory reward you set in `rollout` if present,\n",
        "        # or falls back to its own scoring config.\n",
        "        judged_group = await ruler_score_group(\n",
        "            group,\n",
        "            \"openai/o4-mini\",\n",
        "            debug=False,  # debug=True can produce very large payloads #TODO\n",
        "        )\n",
        "        # Guard against any groups that may come back empty\n",
        "        if len(judged_group.trajectories) > 0:\n",
        "            judged_groups.append(judged_group)\n",
        "\n",
        "    if not judged_groups:\n",
        "        print(\"âš ï¸ No judged groups after RULER, skipping train() this step.\")\n",
        "        continue\n",
        "\n",
        "    # ----------------- 4. Optional validation + logging -----------------\n",
        "    if batch.step % training_config[\"validation_step_interval\"] == 0:\n",
        "        print(\"Running validation at step\", batch.step)\n",
        "\n",
        "        validation_groups = []\n",
        "        for scenario in validation_scenarios:\n",
        "            val_call_scenario = CallScenario(\n",
        "                step=batch.step,\n",
        "                scenario=scenario.model_dump(),\n",
        "            )\n",
        "            # For validation, one rollout per scenario is usually enough\n",
        "            validation_groups.append(\n",
        "                art.TrajectoryGroup(\n",
        "                    [rollout(model, val_call_scenario)]\n",
        "                )\n",
        "            )\n",
        "\n",
        "        finished_validation_groups = await art.gather_trajectory_groups(\n",
        "            validation_groups,\n",
        "            pbar_desc=\"gather-val\",\n",
        "            max_exceptions=len(validation_scenarios),\n",
        "        )\n",
        "\n",
        "        # Filter out empty validation groups\n",
        "        finished_validation_groups = [\n",
        "            g for g in finished_validation_groups if len(g.trajectories) > 0\n",
        "        ]\n",
        "\n",
        "        # Log validation trajectories; if logging fails (e.g. 422),\n",
        "        # we print the error but do not crash training.\n",
        "        if finished_validation_groups:\n",
        "            try:\n",
        "                await model.log(finished_validation_groups, split=\"val\")\n",
        "            except Exception as e:\n",
        "                print(f\"âš ï¸ Validation logging failed: {e}\")\n",
        "\n",
        "    # ----------------- 5. GRPO parameter update -----------------\n",
        "    # Clean up older checkpoints to keep storage usage low\n",
        "    await model.delete_checkpoints()\n",
        "\n",
        "    # Train on the judged groups; if the backend has an internal error,\n",
        "    # we surface it but don't kill the notebook.\n",
        "    try:\n",
        "        await model.train(\n",
        "            judged_groups,\n",
        "            config=art.TrainConfig(\n",
        "                learning_rate=training_config[\"learning_rate\"],\n",
        "            ),\n",
        "        )\n",
        "    except Exception as e:\n",
        "        print(f\"âš ï¸ model.train failed with error: {e}\")\n",
        "        # Optionally `break` here if you want to stop on training errors\n",
        "        break\n",
        "\n",
        "    if batch.step % training_config[\"validation_step_interval\"] == 0 and batch.step > 0:\n",
        "        print(f\"Completed training step {batch.step}\")\n",
        "\n",
        "    # ----------------- 6. Early stop for demo -----------------\n",
        "    if \"max_steps\" in training_config and training_config.get(\"max_steps\") is not None:\n",
        "        print(\"Reached max_steps, stopping training loop.\")\n",
        "        break\n",
        "\n",
        "print(\"âœ… Finished GRPO training loop for call-transcript search agent (RULER).\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZtfRt8dN2sp_"
      },
      "source": [
        "### Using the Model\n",
        "\n",
        "Just like that, you've trained an agent to search emails and answer questions! Now it's time to use your model outside of the training loop.\n",
        "\n",
        "Check out the code below for a small demo of the model you just trained!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zN0eqr-P2sp_",
        "outputId": "340e1ca1-5eaf-494e-f12a-1cfe0b101834"
      },
      "outputs": [],
      "source": [
        "# Test the trained model using the rollout function\n",
        "# This uses the same inference path as training (no special test logic)\n",
        "\n",
        "print(\"Testing the trained model with a real call scenario...\\n\")\n",
        "\n",
        "# Use a scenario from our training set\n",
        "test_scenario = training_scenarios[0]\n",
        "\n",
        "print(f\"Test scenario ID: {test_scenario.id}\")\n",
        "print(f\"Question: {test_scenario.question}\")\n",
        "print(f\"Expected answer: {test_scenario.answer}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Wrap the scenario exactly as expected by rollout\n",
        "test_call_scenario = CallScenario(\n",
        "    step=0,\n",
        "    scenario=test_scenario,\n",
        ")\n",
        "\n",
        "# Run the rollout function with the trained model\n",
        "result_trajectory = await rollout(model, test_call_scenario)\n",
        "\n",
        "print(\"Agent's trajectory:\")\n",
        "print(\"-\" * 20)\n",
        "\n",
        "# Display the conversation (system, user, assistant, tools)\n",
        "messages = result_trajectory.messages()\n",
        "for i, msg in enumerate(messages):\n",
        "    role = msg.get(\"role\", \"unknown\")\n",
        "    content = msg.get(\"content\", \"\")\n",
        "    tool_calls = msg.get(\"tool_calls\", [])\n",
        "\n",
        "    if role == \"system\":\n",
        "        print(\n",
        "            f\"[SYSTEM]: {content[:100]}...\"\n",
        "            if len(content) > 100\n",
        "            else f\"[SYSTEM]: {content}\"\n",
        "        )\n",
        "    elif role == \"user\":\n",
        "        print(f\"[USER]: {content}\")\n",
        "    elif role == \"assistant\":\n",
        "        if tool_calls:\n",
        "            print(f\"[ASSISTANT - TOOL CALLS]: {tool_calls}\")\n",
        "        if content:\n",
        "            print(f\"[ASSISTANT]: {content}\")\n",
        "    elif role == \"tool\":\n",
        "        tool_name = msg.get(\"name\", \"unknown_tool\")\n",
        "        print(\n",
        "            f\"[TOOL - {tool_name}]: {content[:200]}...\"\n",
        "            if len(content) > 200\n",
        "            else f\"[TOOL - {tool_name}]: {content}\"\n",
        "        )\n",
        "\n",
        "    print()\n",
        "\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Print final answer and evaluation\n",
        "if result_trajectory.final_answer:\n",
        "    print(f\"Agent's Final Answer:\\n{result_trajectory.final_answer.answer}\")\n",
        "    print(f\"Source Call IDs Used: {result_trajectory.final_answer.source_ids}\")\n",
        "else:\n",
        "    print(\"No final answer provided by the agent.\")\n",
        "\n",
        "print(f\"\\nExpected Answer:\\n{test_scenario.answer}\")\n",
        "\n",
        "# Optional: show judge scores if available\n",
        "if result_trajectory.metrics:\n",
        "    print(\"\\nEvaluation metrics:\")\n",
        "    for k, v in result_trajectory.metrics.items():\n",
        "        print(f\"  {k}: {v}\")\n",
        "\n",
        "print(\"\\nðŸŽ‰ Call transcript agent testing completed!\")\n",
        "print(\n",
        "    \"The agent was evaluated using the same rollout and judge logic as during training.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "print(\"Testing the trained model with another call scenario...\\n\")\n",
        "\n",
        "# Randomly sample a scenario (instead of always picking index 0)\n",
        "test_scenario = random.choice(validation_scenarios)\n",
        "\n",
        "print(f\"Test scenario ID: {test_scenario.id}\")\n",
        "print(f\"Question: {test_scenario.question}\")\n",
        "print(f\"Expected answer: {test_scenario.answer}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Wrap the scenario exactly as rollout expects\n",
        "test_call_scenario = CallScenario(\n",
        "    step=0,\n",
        "    scenario=test_scenario,\n",
        ")\n",
        "\n",
        "# Run rollout (same inference path as training)\n",
        "result_trajectory = await rollout(model, test_call_scenario)\n",
        "\n",
        "print(\"Agent's trajectory:\")\n",
        "print(\"-\" * 20)\n",
        "\n",
        "# Display the full conversation\n",
        "for msg in result_trajectory.messages():\n",
        "    role = msg.get(\"role\", \"unknown\")\n",
        "    content = msg.get(\"content\", \"\")\n",
        "    tool_calls = msg.get(\"tool_calls\", [])\n",
        "\n",
        "    if role == \"system\":\n",
        "        print(\n",
        "            f\"[SYSTEM]: {content[:120]}...\"\n",
        "            if len(content) > 120\n",
        "            else f\"[SYSTEM]: {content}\"\n",
        "        )\n",
        "    elif role == \"user\":\n",
        "        print(f\"[USER]: {content}\")\n",
        "    elif role == \"assistant\":\n",
        "        if tool_calls:\n",
        "            print(f\"[ASSISTANT - TOOL CALLS]: {tool_calls}\")\n",
        "        if content:\n",
        "            print(f\"[ASSISTANT]: {content}\")\n",
        "    elif role == \"tool\":\n",
        "        tool_name = msg.get(\"name\", \"unknown_tool\")\n",
        "        print(\n",
        "            f\"[TOOL - {tool_name}]: {content[:300]}...\"\n",
        "            if len(content) > 300\n",
        "            else f\"[TOOL - {tool_name}]: {content}\"\n",
        "        )\n",
        "\n",
        "    print()\n",
        "\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Final answer summary\n",
        "if result_trajectory.final_answer:\n",
        "    print(\"Agent's Final Answer:\")\n",
        "    print(result_trajectory.final_answer.answer)\n",
        "    print(f\"Source Call IDs Used: {result_trajectory.final_answer.source_ids}\")\n",
        "else:\n",
        "    print(\"âš ï¸ No final answer provided by the agent.\")\n",
        "\n",
        "print(\"\\nExpected Answer:\")\n",
        "print(test_scenario.answer)\n",
        "\n",
        "# Optional: show grading details\n",
        "if result_trajectory.metrics:\n",
        "    print(\"\\nEvaluation metrics:\")\n",
        "    for k, v in result_trajectory.metrics.items():\n",
        "        print(f\"  {k}: {v}\")\n",
        "\n",
        "print(\"\\nâœ… Call transcript agent test completed.\")\n",
        "print(\n",
        "    \"This run used the same rollout, tools, and scoring logic as training \"\n",
        "    \"to ensure behavior is representative of learned policy.\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Test the trained model using the rollout function\n",
        "# This uses the same inference path as training (no special test logic)\n",
        "\n",
        "print(\"Testing the trained model with a real call scenario...\\n\")\n",
        "\n",
        "# Use a scenario from our training set\n",
        "test_scenario = training_scenarios[0]\n",
        "\n",
        "print(f\"Test scenario ID: {test_scenario.id}\")\n",
        "print(f\"Question: {test_scenario.question}\")\n",
        "print(f\"Expected answer: {test_scenario.answer}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Wrap the scenario exactly as expected by rollout\n",
        "test_call_scenario = CallScenario(\n",
        "    step=0,\n",
        "    scenario=test_scenario,\n",
        ")\n",
        "\n",
        "# Run the rollout function with the trained model\n",
        "result_trajectory = await rollout(model, test_call_scenario)\n",
        "\n",
        "print(\"Agent's trajectory:\")\n",
        "print(\"-\" * 20)\n",
        "\n",
        "# Display the conversation (system, user, assistant, tools)\n",
        "messages = result_trajectory.messages()\n",
        "for i, msg in enumerate(messages):\n",
        "    role = msg.get(\"role\", \"unknown\")\n",
        "    content = msg.get(\"content\", \"\")\n",
        "    tool_calls = msg.get(\"tool_calls\", [])\n",
        "\n",
        "    if role == \"system\":\n",
        "        print(\n",
        "            f\"[SYSTEM]: {content[:100]}...\"\n",
        "            if len(content) > 100\n",
        "            else f\"[SYSTEM]: {content}\"\n",
        "        )\n",
        "    elif role == \"user\":\n",
        "        print(f\"[USER]: {content}\")\n",
        "    elif role == \"assistant\":\n",
        "        if tool_calls:\n",
        "            print(f\"[ASSISTANT - TOOL CALLS]: {tool_calls}\")\n",
        "        if content:\n",
        "            print(f\"[ASSISTANT]: {content}\")\n",
        "    elif role == \"tool\":\n",
        "        tool_name = msg.get(\"name\", \"unknown_tool\")\n",
        "        print(\n",
        "            f\"[TOOL - {tool_name}]: {content[:200]}...\"\n",
        "            if len(content) > 200\n",
        "            else f\"[TOOL - {tool_name}]: {content}\"\n",
        "        )\n",
        "\n",
        "    print()\n",
        "\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# Print final answer and evaluation\n",
        "if result_trajectory.final_answer:\n",
        "    print(f\"Agent's Final Answer:\\n{result_trajectory.final_answer.answer}\")\n",
        "    print(f\"Source Call IDs Used: {result_trajectory.final_answer.source_ids}\")\n",
        "else:\n",
        "    print(\"No final answer provided by the agent.\")\n",
        "\n",
        "print(f\"\\nExpected Answer:\\n{test_scenario.answer}\")\n",
        "\n",
        "# Optional: show judge scores if available\n",
        "if result_trajectory.metrics:\n",
        "    print(\"\\nEvaluation metrics:\")\n",
        "    for k, v in result_trajectory.metrics.items():\n",
        "        print(f\"  {k}: {v}\")\n",
        "\n",
        "print(\"\\nðŸŽ‰ Call transcript agent testing completed!\")\n",
        "print(\n",
        "    \"The agent was evaluated using the same rollout and judge logic as during training.\"\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "python_uv_3_11_mk2",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
